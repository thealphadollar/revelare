
<!-- Post Layout Start -->

<!DOCTYPE html>
<html lang="en">

  
<!-- HEAD Start -->

<head>
  


  <!-- Force HTTPS Start -->
  <script>
  // Don't force http when serving the website locally
  if (!(window.location.host.startsWith("127.0.0.1")) && (window.location.protocol != "https:"))
    window.location.protocol = "https";
  </script>

  <!-- Force HTTPS End -->



  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>How I Handled 1 TB of Data With Python! | { Cogito, Ergo Sum }</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="How I Handled 1 TB of Data With Python!" />
<meta name="author" content="thealphadollar" />
<meta property="og:locale" content="en" />
<meta name="description" content="I recently had the opportunity to work on a large dataset (roughly 1TB) and analyze it using Python for filtering data based on some given parameters. In this post, I’ll be summing up how I accomplished the task using Python and used various measures to make the filtering process efficient." />
<meta property="og:description" content="I recently had the opportunity to work on a large dataset (roughly 1TB) and analyze it using Python for filtering data based on some given parameters. In this post, I’ll be summing up how I accomplished the task using Python and used various measures to make the filtering process efficient." />
<link rel="canonical" href="https://thealphadollar.me/learning/2020/02/23/large-data-analytics.html" />
<meta property="og:url" content="https://thealphadollar.me/learning/2020/02/23/large-data-analytics.html" />
<meta property="og:site_name" content="{ Cogito, Ergo Sum }" />
<meta property="og:image" content="https://thealphadollar.me/img/posts/aws-1tb/bigdata.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-23T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://thealphadollar.me/img/posts/aws-1tb/bigdata.jpg" />
<meta property="twitter:title" content="How I Handled 1 TB of Data With Python!" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"thealphadollar"},"dateModified":"2020-02-23T00:00:00+00:00","datePublished":"2020-02-23T00:00:00+00:00","description":"I recently had the opportunity to work on a large dataset (roughly 1TB) and analyze it using Python for filtering data based on some given parameters. In this post, I’ll be summing up how I accomplished the task using Python and used various measures to make the filtering process efficient.","headline":"How I Handled 1 TB of Data With Python!","image":"https://thealphadollar.me/img/posts/aws-1tb/bigdata.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://thealphadollar.me/learning/2020/02/23/large-data-analytics.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://thealphadollar.me/img/author_logo.png"},"name":"thealphadollar"},"url":"https://thealphadollar.me/learning/2020/02/23/large-data-analytics.html"}</script>
<!-- End Jekyll SEO tag -->

  <meta name="keywords" content="thealphadollar, thealpha$, personal, shivam kumar jha, shivam, thealpha, shivam jha, mercari, iitkgp student">
  <!-- Bootstrap Core Customized CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">

  <!-- Custom CSS -->
  <link href="/css/grayscale.css" rel="stylesheet">

  <!-- Custom Fonts -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/rrssb.css" />
  <!-- CSS for responsive video through youtubePlayer.html -->
  <link rel="stylesheet" href="/css/youtubePlayer.css" />
  <!-- CSS for caption -->
  <link rel="stylesheet" href="/css/imageCaption.css" />
  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->
  
    <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico">
  

  

  



  
<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">


  


  <!-- Syntax highlight in post pages -->

  <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.2/styles/monokai_sublime.min.css">




</head>

<!-- HEAD End -->


  <body>

    
<!-- Navigation Start -->

<nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
        <i class="fa fa-bars"></i>
      </button>
      
        <a class="navbar-brand" href="/">
      
          <div>
            
              <img src="/img/favicon.ico" alt="" style="margin-top: -5%; vertical-align:top;">
            
            { Cogito, Ergo Sum }
          </div>
        </a>
    </div>
    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
      <ul class="nav navbar-nav">
        
          <!-- Blog, Post, Tag and Error pages -->
          
            <li>
            
                <a href="/#about"> About </a>
            
            </li>
          
            <li>
            
              
                <a href="/blog/"> Blog </a>
              
            
            </li>
          
            <li>
            
                <a href="/#tags"> Tags </a>
            
            </li>
          
            <li>
            
                <a href="/#connect"> Connect </a>
            
            </li>
          
        
      </ul>
    </div>
  </div>
</nav>

<!-- Navigation End -->


    <section id="post" class="container content-section text-center">
      <div class="row">
        <div class="col-md-10 col-md-offset-1">

          
<!-- Swipe Instructions Start -->

<div id="swipe-instruction">
  <div>
    <p><br><br><br></p>
    <i id="hand-swipe" class="fa fa-hand-o-up"></i>
    <p><strong>
    
      Did you know that you can navigate the posts by swiping left and right?
    
    </strong></p>
    <button type="button" class="btn btn-default ok-btn close-swipe-instruction">
      
        Awesome!
      
    </button>
  </div>
</div>

<!-- Swipe Instructions End -->


          <h1 style="margin-bottom: -.3em; margin-top: -3em; color: #64ffda;"><strong>How I Handled 1 TB of Data With Python!</strong></h1>
          <hr style="border-top: 3px dashed #64ffda; margin-bottom: 3em;">
          <h4>
            <strong>23 Feb 2020</strong>
            <small>
              . <strong><span class="reading-time" title="Estimated read time">
  
  
    13 mins
  
</span></strong>:
              <a class="category" href="/categories/learning.html">
                learning
              </a>.
              <a href="/learning/2020/02/23/large-data-analytics.html#disqus_thread">Comments</a>
              <br>
              
                <a class="tag" href="/tags/bigdata.html">#bigdata</a>
              
                <a class="tag" href="/tags/aws.html">#aws</a>
              
                <a class="tag" href="/tags/experience.html">#experience</a>
              
                <a class="tag" href="/tags/coding.html">#coding</a>
              
                <a class="tag" href="/tags/data%20analytics.html">#data analytics</a>
              
                <a class="tag" href="/tags/guide.html">#guide</a>
              
                <a class="tag" href="/tags/lessons.html">#lessons</a>
              
                <a class="tag" href="/tags/programming.html">#programming</a>
              
                <a class="tag" href="/tags/python.html">#python</a>
              
                <a class="tag" href="/tags/tech.html">#tech</a>
              
                <a class="tag" href="/tags/tutorial.html">#tutorial</a>
              
            </small>
          </h4>

          <section class="text-justify">
            <p>I recently had the opportunity to work on a large dataset (roughly <strong>1TB</strong>) and analyze it using Python for filtering data based on some given parameters. In this post, I’ll be summing up how I accomplished the task using Python and used various measures to make the filtering process efficient.</p>

<h3 id="task-details">Task Details</h3>

<p><img src="https://thealphadollar.me/img/posts/aws-1tb/python-aws.jpeg" alt="Python AWS"></p>

<p>We have a large number of gzip files inside folders in the root folder in a AWS S3 bucket and we have a EC2 instance which has been <a href="https://www.youtube.com/watch?v=NHAuCWIHevk">given IAM role to access the bucket</a> in read-write mode. Each of the compressed file contains a JSON file which has a Python dictionary in each of it’s line. Note that this is <strong>not</strong> our typical JSON file which we can load entirely; <strong>each of the line represents a Python object</strong> (dictionary here) but the entire file doesn’t represent an object. Refer below for better understanding.</p>

<pre><code class="language-JSON">{'key1': 'value1', 'key2': 2}
{'key1': 'value2', 'key2': 3}

</code></pre>

<p>An important aspect is that each of the compressed file is around 1 GB and after decompressing it gives us a file of size more than 4 GB. For starters, we have an EC2 instance with just 2GB of memory and hence we can’t be loading the entire file at once into the program lest we want to face memory issues.</p>

<p>The output of filtering should be a CSV file with matching entries based on a filter. The caveat here is that we <strong>do not</strong> know about the number of entries that may match our filter and hence we can neither store this file in the memory nor directly write it to the disk since we have an instance with just 8GB of disk storage.</p>

<p>So, let’s move on and see how we can solve the problems we have here.</p>

<h4 id="attaching-s3-as-file-system">Attaching S3 as File System</h4>

<p>The first step we should do which will ease a lot of stuff for us is attaching the S3 bucket as a file system in our EC2 instance. This is very easy if we are trying to access the bucket from an instance of EC2 which has been granted the IAM role for the bucket you are trying to access. A great python package available for the same is <a href="https://pypi.org/project/s3fs/">s3fs</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">s3fs</span>

<span class="n">fs</span> <span class="o">=</span> <span class="n">s3fs</span><span class="p">.</span><span class="n">S3FileSystem</span><span class="p">(</span><span class="n">anon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1"># opening an example file
</span><span class="k">with</span> <span class="n">fs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"bucket/file.gz"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f_gz</span><span class="p">:</span>
	<span class="k">pass</span>

</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">anon</code> is False since we will be authenticating based on the IAM role attached to the EC2 instance.</p>

<p>With the successful attachment of the bucket, we can use it just like our own file system. So, on an abstration level we can say that it is like mounting an external hard drive. Also, since S3 is being accessed by EC2 the speeds are pretty incredible and the bottleneck is minimal.</p>

<h4 id="handling-data-output">Handling Data Output</h4>

<p><img src="https://thealphadollar.me/img/posts/aws-1tb/sf3s.png" alt="S3FS"></p>

<p>As we noted earlier, we are not sure about the size of the filtered data that our program will output and hence we cannot keep the same on our hard drive storage (i.e. 8GB). For this, we need to use the attached S3 and create a file in which we would be writing as each of the line is spewed out by our program.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">csv</span>

<span class="k">with</span> <span class="n">fs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'bucket/output.csv'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f_csv</span><span class="p">:</span>
	<span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="p">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">f_csv</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">output_object</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
	<span class="c1"># write header
</span>	<span class="n">writer</span><span class="p">.</span><span class="n">writeheader</span><span class="p">()</span>
	<span class="k">while</span><span class="p">(</span><span class="bp">True</span><span class="p">):</span>
		<span class="c1"># --- logic to read line by line simplified
</span>		<span class="n">line</span> <span class="o">=</span> <span class="n">process_input</span><span class="p">()</span>
		<span class="n">writer</span><span class="p">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
		<span class="p">...</span>

</code></pre></div></div>

<p>The above code has two points which are important; firstly, we <strong>open the file only once</strong> and do all the work and at the end we come out of the scope of <code class="language-plaintext highlighter-rouge">with</code>, closing the file. This helps us avoid the <a href="https://stackoverflow.com/questions/11349020/python-file-open-close-every-time-vs-keeping-it-open-until-the-process-is-finish">extra time that it’ll take us to open and close the file each time</a> we are writing to it (remember we are talking about potentially millions of lines). It is important to notice that this doesn’t mean that the writings are kept in the memory and then passed on when we close it. The writing is happening as we produce each line, just that the file object is kept alive all of that time.</p>

<p>Second important point is that since this is a CSV file, we need to have the same structure for all the objects we will be trying to write into this file. For the same purpose we will be using a dictionary <code class="language-plaintext highlighter-rouge">output_object</code>. We will be keeping this as a OrderedDict in order to avoid reordering of the keys in each new instance of the object. Whenever creating a new output object, we will be using python <a href="https://www.geeksforgeeks.org/copy-python-deep-copy-shallow-copy/">DeepCopy</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>

<span class="n">new_output</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">output_object</span><span class="p">)</span>

</code></pre></div></div>

<p>The alternate could  be shallow copy (<code class="language-plaintext highlighter-rouge">copy</code> instead of <code class="language-plaintext highlighter-rouge">deepcopy</code>) if the object just has single level of depth.</p>

<h4 id="handling-data-input">Handling Data Input</h4>

<p>Data of 1TB is not easy to handle, and we will be implementing a few measures to make sure we do not run out of memory while handling the data as well as making sure the process is efficient enough. Firstly, let’s talk about how are we going to load the data: we will be listing each of the folder in the root folder and then iteratively downloading one file at a time to our system (we will see how this is better than simply using <code class="language-plaintext highlighter-rouge">s3fs</code> in a while).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all_folders_in_root</span> <span class="o">=</span> <span class="n">fs</span><span class="p">.</span><span class="n">ls</span><span class="p">(</span><span class="n">ROOT_FOLDER</span><span class="p">)</span>
<span class="k">for</span> <span class="n">folder</span> <span class="ow">in</span> <span class="n">all_folders_in_root</span><span class="p">:</span>
	<span class="n">files</span> <span class="o">=</span> <span class="n">fs</span><span class="p">.</span><span class="n">ls</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
		<span class="n">fs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s">'data.gz'</span><span class="p">)</span>
		<span class="c1"># work with file here
</span>		<span class="p">...</span>
		<span class="c1"># work done
</span>		<span class="n">os</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="s">'data.gz'</span><span class="p">)</span>

</code></pre></div></div>

<p>This makes sure that we are not downloading all the files and storing them which would have led us to run out of disk space. Next comes another crucial step; remember that these files are gzip compressed files and so you might think that we need to decompress them before reading the JSON file line by line. NO, we can read it with <code class="language-plaintext highlighter-rouge">gzip</code> python package as normal files without decompression. This is the reason we had to download the entire file instead of reading it using <code class="language-plaintext highlighter-rouge">s3fs</code> since this method will only work on files on the disk.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all_folders_in_root</span> <span class="o">=</span> <span class="n">fs</span><span class="p">.</span><span class="n">ls</span><span class="p">(</span><span class="n">ROOT_FOLDER</span><span class="p">)</span>
<span class="k">for</span> <span class="n">folder</span> <span class="ow">in</span> <span class="n">all_folders_in_root</span><span class="p">:</span>
	<span class="n">files</span> <span class="o">=</span> <span class="n">fs</span><span class="p">.</span><span class="n">ls</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
		<span class="n">fs</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s">'data.gz'</span><span class="p">)</span>
		<span class="c1"># work with file here
</span>		<span class="k">with</span> <span class="n">gzip</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'data.gz'</span><span class="p">,</span> <span class="s">'rt'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f_gz</span><span class="p">:</span>
			<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fgz</span><span class="p">:</span>
				<span class="n">process_line</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">csv</span><span class="p">,</span> <span class="n">csv_f</span><span class="p">)</span>
		<span class="c1"># work done
</span>		<span class="n">os</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="s">'data.gz'</span><span class="p">)</span>

</code></pre></div></div>

<p>‘rt’ implies that we are reading the file in text mode. A list of all acceptable modes is available <a href="https://www.reddit.com/r/learnpython/comments/88h4yz/what_do_the_gzipopen_modes_mean/">here</a>. This way we are able to read it just like if we had the file uncompressed and since we are still reading it line by line, the entire file is not loaded into the memory and we do not have to deal with file size problems and low memory issues.</p>

<h5 id="handling-filtering">Handling Filtering</h5>

<p>This is a subset of how we are going to handle the data input and deals with good practices for comparison in order to reduce time complexity of operations since we will be doing the same operations over and over on thousands of millions of lines. Firstly, we will be using <a href="https://pypi.org/project/ujson/"><code class="language-plaintext highlighter-rouge">ujson</code></a> package which uses Cython at the backside in order to provide <a href="https://medium.com/dataweave/json-vs-simplejson-vs-ujson-2887b2c128b2">speed improvements of four times overall inbuilt <code class="language-plaintext highlighter-rouge">json</code> library</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">ujson</span>

<span class="n">ujson</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">object_str</span><span class="p">)</span>

</code></pre></div></div>

<p>The above example shows how to load a string which represents a valid python object using <code class="language-plaintext highlighter-rouge">ujson</code>.</p>

<p>Now that we have sped up on the loading of objects, let’s think about comparisons. Since we have been saving the memory everywhere in this program, we can splurge on a bit of memory now. We will be storing all the objects that we need to compare in or check presence in as dictionaries. We will discuss why, shortly! Below is a small snippet which checks for the presence of “key2” in the dictionary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dict_A</span> <span class="o">=</span> <span class="p">{</span><span class="s">'key1'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'key2'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">'key3'</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="k">if</span> <span class="s">'key2'</span> <span class="ow">in</span> <span class="n">dict_A</span><span class="p">:</span>
	<span class="c1"># do something
</span>
</code></pre></div></div>

<p>We could have implemented the above as a list of ‘key1’, ‘key2’ and ‘key3’ and then checked for the presence of ‘key2’ in that. Can you see the problem in that? It would have been a linear search while what we just did above is a constant time search since the keys of a dictionary are hashed and looked up using hash tables (this is for example, the actual implementation may differ). We have just saved ourselves so much of time otherwise this linear algorithm would have been executed thousands of millions of times, completing ruining our dreams of an efficient program. We could have forgotten about getting the result at all within the next few weeks!</p>

<p>NOTE: If you have to use lists, use them after converting to <code class="language-plaintext highlighter-rouge">set</code> since <a href="https://www.quora.com/Why-is-a-set-faster-than-a-list-in-Python">membership tests are much faster in a set than list</a>.</p>

<h4 id="other-small-improvements-and-details">Other Small Improvements And Details</h4>

<p>With the last discussion, we have come to an end and completed all steps we should be following in creating an efficient enough program for handling humongous files which neither fit in our disk nor memory. Can we do it better? Yes, definitely! I’ll be listing a few points below which we can incorporate in order to make the program run faster.</p>

<ul>
  <li>Completely avoid use of Global variables and pass variables as arguments or use local variables whenever necessary. <a href="https://stackoverflow.com/questions/12590058/python-performance-with-global-variables-vs-local">Source</a>
</li>
  <li>Try to avoid one liners and make variable assignments wherever necessary, this is counterintuitive but works how Python is implemented. <a href="https://stackoverflow.com/questions/36548518/why-is-code-using-intermediate-variables-faster-than-code-without/36549633">Source</a>
</li>
  <li>Use Cython packages wherever possible and the speed is crucial. I’ve not used it extensively above to keep the article simple. <a href="http://okigiveup.net/an-introduction-to-cython/">Learn More</a>
</li>
  <li>Use buffers instead of reading and writing one line at a time. We could have kept a fixed size buffer and then loaded N (some large number) lines, processed them, got the output as M (some large number) lines and then written them and repeated the cycle. <a href="https://stackoverflow.com/questions/49266939/time-performance-in-generating-very-large-text-file-in-python">Source</a>.</li>
</ul>

<p>If you are interested in few more speed tips, <a href="https://wiki.python.org/moin/PythonSpeed">here</a> is a list of other speed improvements one can look at.</p>

<h2 id="conclusion">Conclusion</h2>
<p>It is always fun to push your limits and learn better ways of using a language. Before this project came to me, I had never worked on such a huge dataset and, though the task seemed infeasible at the start, was completed with 5 hours of computation time.</p>

<p>Please feel free to provide feedback below and ask queries if you have any <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> Also, if you can provide any better alternatives to the methods used above, please post below.</p>

          </section>

          
<!-- Share Buttons Start -->
<div>
  <ul class="rrssb-buttons clearfix">
    
    
      <li class="rrssb-facebook">
        <a href="https://www.facebook.com/sharer/sharer.php?u=https://thealphadollar.me/learning/2020/02/23/large-data-analytics.html&amp;title=How%20I%20Handled%201%20TB%20of%20Data%20With%20Python!" class="popup">
          <span class="rrssb-icon">
            <svg xmlns="https://www.w3.org/2000/svg" preserveaspectratio="xMidYMid" width="29" height="29" viewbox="0 0 29 29"><path d="M26.4 0H2.6C1.714 0 0 1.715 0 2.6v23.8c0 .884 1.715 2.6 2.6 2.6h12.393V17.988h-3.996v-3.98h3.997v-3.062c0-3.746 2.835-5.97 6.177-5.97 1.6 0 2.444.173 2.845.226v3.792H21.18c-1.817 0-2.156.9-2.156 2.168v2.847h5.045l-.66 3.978h-4.386V29H26.4c.884 0 2.6-1.716 2.6-2.6V2.6c0-.885-1.716-2.6-2.6-2.6z" class="cls-2" fill-rule="evenodd"></path></svg>
          </span>
          <span class="rrssb-text">facebook</span>
        </a>
      </li>
    
    
      <li class="rrssb-twitter">
        <a href="https://twitter.com/share?url=https://thealphadollar.me/learning/2020/02/23/large-data-analytics.html&amp;text=How%20I%20Handled%201%20TB%20of%20Data%20With%20Python!" class="popup">
          <span class="rrssb-icon"><svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewbox="0 0 28 28"><path d="M24.253 8.756C24.69 17.08 18.297 24.182 9.97 24.62c-3.122.162-6.22-.646-8.86-2.32 2.702.18 5.375-.648 7.507-2.32-2.072-.248-3.818-1.662-4.49-3.64.802.13 1.62.077 2.4-.154-2.482-.466-4.312-2.586-4.412-5.11.688.276 1.426.408 2.168.387-2.135-1.65-2.73-4.62-1.394-6.965C5.574 7.816 9.54 9.84 13.802 10.07c-.842-2.738.694-5.64 3.434-6.48 2.018-.624 4.212.043 5.546 1.682 1.186-.213 2.318-.662 3.33-1.317-.386 1.256-1.248 2.312-2.4 2.942 1.048-.106 2.07-.394 3.02-.85-.458 1.182-1.343 2.15-2.48 2.71z"></path></svg></span>
          <span class="rrssb-text">twitter</span>
        </a>
      </li>
    
    
      <li class="rrssb-linkedin">
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://thealphadollar.me/learning/2020/02/23/large-data-analytics.html" class="popup">
        <span class="rrssb-icon">
          <svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewbox="0 0 28 28">
            <path d="M25.424 15.887v8.447h-4.896v-7.882c0-1.98-.71-3.33-2.48-3.33-1.354 0-2.158.91-2.514 1.802-.13.315-.162.753-.162 1.194v8.216h-4.9s.067-13.35 0-14.73h4.9v2.087c-.01.017-.023.033-.033.05h.032v-.05c.65-1.002 1.812-2.435 4.414-2.435 3.222 0 5.638 2.106 5.638 6.632zM5.348 2.5c-1.676 0-2.772 1.093-2.772 2.54 0 1.42 1.066 2.538 2.717 2.546h.032c1.71 0 2.77-1.132 2.77-2.546C8.056 3.593 7.02 2.5 5.344 2.5h.005zm-2.48 21.834h4.896V9.604H2.867v14.73z"></path>
          </svg>
        </span>
        <span class="rrssb-text">linkedin</span>
      </a>
      </li>
    
    
    
    
    
    
    
  </ul>
</div>

<!-- Share Buttons End -->


          
            
<!-- Disqus Comments Start -->


  <div id="disqus_thread"></div>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>


<!-- Disqus Comments End -->

          


          <hr>

          
          <div class="author row">
            <img class="col-xs-4 col-sm-3 col-md-2" src="/img/author.png" alt="Me">
            <p class="col-xs-8 col-sm-9 col-md-10">
              I'm intrigued by human psychology and code backend for videogames. I live with a philosophy to be simple, true, and kind always. I enjoy taking days slowly and writing when I learn something new - ah! that reminds me, I enjoy learning from new experiences a lot.
            </p>
          </div>
          
        </div>
      </div>
    </section>

    <!-- Footer Start -->

<footer>

    <!-- Social Buttons Start -->

<ul class="list-inline social-buttons">
    
    <li><a href="https://facebook.com/thealphadollar" target="_blank"><i class="fa fa-facebook fa-fw"></i></a></li>
    
    <li><a href="https://github.com/thealphadollar" target="_blank"><i class="fa fa-github fa-fw"></i></a></li>
    
    <li><a href="https://www.linkedin.com/in/thealphadollar" target="_blank"><i class="fa fa-linkedin fa-fw"></i></a></li>
    
    
</ul>

<!-- Social Buttons End -->


    <p><br><br></p>

    <div class="container text-center">
        <p>Copyright © thealphadollar 2023</p>
        
    </div>
</footer>

<p><br><br><br><br><br><br></p>

<!-- Footer End -->


    
<!-- Javascript Start -->

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

<!-- Plugin JavaScript -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

<script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 56,
  backgroundColor: 'rgb(0,205,255)',
  textColor: '#000'
})</script>

<!-- Custom Theme JavaScript -->
<!--* Start Bootstrap - Grayscale Bootstrap Theme (http://startbootstrap.com)
* Code licensed under the Apache License v2.0.
* For details, see http://www.apache.org/licenses/LICENSE-2.0.-->
<script>
function toggleNavCollapse(){50<$(".navbar").offset().top?$(".navbar-fixed-top").addClass("top-nav-collapse"):$(".navbar-fixed-top").removeClass("top-nav-collapse");}
$(document).ready(toggleNavCollapse);
$(window).scroll(toggleNavCollapse);$(function(){$("a.page-scroll").bind("click",function(b){var a=$(this);$("html, body").stop().animate({scrollTop:$(a.attr("href")).offset().top-50},1500,"easeInOutExpo",function(){a.blur()});b.preventDefault()})});$(".navbar-collapse ul li a").click(function(){$(".navbar-toggle:visible").click()});
</script>





  <!-- Syntax highlight in post pages-->

  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.2/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>




  

  <!-- Google Tracking Id Start -->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-121294295-1', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- Google Tracking Id End -->

  




  <!-- Disqus -->

  

    <script type="text/javascript">
    var disqus_shortname = 'https-thealphathealphadollardollar-github-io';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>

  

  

    <!-- Comments Counter Start -->

    <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'https-thealphathealphadollardollar-github-io'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
    </script>

    <!-- Comments Counter End -->

  





  <!-- Share buttons Start -->

  <script src="/js/rrssb.min.js"></script>

  <!-- Share buttons End -->





<script>
function addTohistory() {
  if (!window.location.host.startsWith("127.0.0.1")) {
    history.pushState({}, 'How I Handled 1 TB of Data With Python!', 'https://thealphadollar.me/learning/2020/02/23/large-data-analytics.html');
  }
}
</script>

<!-- Gesture Navigation / Swipe Instruction Start -->


<!-- Javascript End -->


  </body>
</html>

<!-- Post Layout End -->
